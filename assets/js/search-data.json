{
  
    
        "post0": {
            "title": "System Identification with Machine Learning Models - Part 1",
            "content": "Photo by Pascal Müller on Unsplash . Recently, I have been looking at ways to create synthetic datasets, specificaly from timeseries given a real measured dataset using Machine Learning models. So, the basic idea is to use ML models for system identifiction. That means to characterize the dynamics of a system using measurements of the inputs and outputs of the system. . One of the most popular non-linear system identification approach is based on the application of the Artificial Neural Networks (ANNs) (Gupta et al., 2003; Mueller and Lemke, 2000; Nelles, 2001) . Typically, in a dynamic system the values of the output variables depend on both the instantaneous values of the input variables and also on the past behavious of the system. . In this notebook (part 1 of upcoming notebook posts), I&#39;ll start with simple ML models that only take into account the instantaneous values of the input variables. For this purpose, I searched through Kaggle and picked a time series dataset that has seasonal and environmental input variables. The dataset is Bike Share Daily Data from Hadi Fanaee-T, Laboratory of Artificial Intelligence and Decision Support (LIAAD), University of Porto. . Bike Share Daily Dataset . Here is the description of the dataset according to their Kaggle page: . Bike-sharing rental process is highly correlated to the environmental and seasonal settings. For instance, weather conditions, precipitation, day of week, season, hour of the day, etc. can affect the rental behaviors. The core data set is related to the two-year historical log corresponding to years 2011 and 2012 from Capital Bikeshare system, Washington D.C., USA which is publicly available in http://capitalbikeshare.com/system-data. We aggregated the data on two hourly and daily basis and then extracted and added the corresponding weather and seasonal information. Weather information are extracted from http://www.freemeteo.com. And here is the description of the fields in the dataset: . instant: record index | dteday : date | season : season (1:springer, 2:summer, 3:fall, 4:winter) | yr : year (0: 2011, 1:2012) | mnth : month ( 1 to 12) | holiday : weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule) | weekday : day of the week | workingday : if day is neither weekend nor holiday is 1, otherwise is 0. | weathersit : 1: Clear, Few clouds, Partly cloudy, Partly cloudy | 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist | 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds | 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog | . | temp : Normalized temperature in Celsius. The values are divided to 41 (max) | atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max) | hum: Normalized humidity. The values are divided to 100 (max) | windspeed: Normalized wind speed. The values are divided to 67 (max) | casual: count of casual users | registered: count of registered users | cnt: count of total rental bikes including both casual and registered | . In this notebook, we will train a model that characterize the dynamics of bike rental behaviour given the environmental and seasonal settings. We can look at this as a prediction problem where we try to predict the bike user counts (casual and registered) given the input data. . After a short EDA and data preparation, we&#39;ll be using Gradient Boosting, Random Forest, and a simple DNN for this prediction and modeling problem. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor from sklearn.multioutput import MultiOutputRegressor from sklearn.metrics import mean_absolute_error import tensorflow as tf . Loading the data . According to the description of the dataset, the enviromental features (4 columns of temp, atemp, hum and windspeed) are normalized. Since, we will standardize all continuous valued features later, we can reverse the normalization and turn according to the description below from the dataset owner: . temp : Normalized temperature in Celsius. The values are divided to 41 (max) | hum: Normalized humidity. The values are divided to 100 (max) | windspeed: Normalized wind speed. The values are divided to 67 (max) | atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max) | . df = pd.read_csv(&#39;bike_sharing_daily.csv&#39;, index_col =&#39;dteday&#39;) df.drop(&#39;instant&#39;, axis=1, inplace=True) df[&#39;temp&#39;] = df[&#39;temp&#39;]*41 df[&#39;atemp&#39;] = df[&#39;atemp&#39;]*50 df[&#39;hum&#39;] = df[&#39;hum&#39;]*100 df[&#39;windspeed&#39;] = df[&#39;windspeed&#39;]*100 df . season yr mnth holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt . dteday . 2011-01-01 1 | 0 | 1 | 0 | 6 | 0 | 2 | 14.110847 | 18.18125 | 80.5833 | 16.0446 | 331 | 654 | 985 | . 2011-01-02 1 | 0 | 1 | 0 | 0 | 0 | 2 | 14.902598 | 17.68695 | 69.6087 | 24.8539 | 131 | 670 | 801 | . 2011-01-03 1 | 0 | 1 | 0 | 1 | 1 | 1 | 8.050924 | 9.47025 | 43.7273 | 24.8309 | 120 | 1229 | 1349 | . 2011-01-04 1 | 0 | 1 | 0 | 2 | 1 | 1 | 8.200000 | 10.60610 | 59.0435 | 16.0296 | 108 | 1454 | 1562 | . 2011-01-05 1 | 0 | 1 | 0 | 3 | 1 | 1 | 9.305237 | 11.46350 | 43.6957 | 18.6900 | 82 | 1518 | 1600 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2012-12-27 1 | 1 | 12 | 0 | 4 | 1 | 2 | 10.420847 | 11.33210 | 65.2917 | 35.0133 | 247 | 1867 | 2114 | . 2012-12-28 1 | 1 | 12 | 0 | 5 | 1 | 2 | 10.386653 | 12.75230 | 59.0000 | 15.5471 | 644 | 2451 | 3095 | . 2012-12-29 1 | 1 | 12 | 0 | 6 | 0 | 2 | 10.386653 | 12.12000 | 75.2917 | 12.4383 | 159 | 1182 | 1341 | . 2012-12-30 1 | 1 | 12 | 0 | 0 | 0 | 1 | 10.489153 | 11.58500 | 48.3333 | 35.0754 | 364 | 1432 | 1796 | . 2012-12-31 1 | 1 | 12 | 0 | 1 | 1 | 2 | 8.849153 | 11.17435 | 57.7500 | 15.4846 | 439 | 2290 | 2729 | . 731 rows × 14 columns . df.describe().T . count mean std min 25% 50% 75% max . season 731.0 | 2.496580 | 1.110807 | 1.000000 | 2.000000 | 3.000000 | 3.000000 | 4.000000 | . yr 731.0 | 0.500684 | 0.500342 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | 1.000000 | . mnth 731.0 | 6.519836 | 3.451913 | 1.000000 | 4.000000 | 7.000000 | 10.000000 | 12.000000 | . holiday 731.0 | 0.028728 | 0.167155 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | . weekday 731.0 | 2.997264 | 2.004787 | 0.000000 | 1.000000 | 3.000000 | 5.000000 | 6.000000 | . workingday 731.0 | 0.683995 | 0.465233 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | 1.000000 | . weathersit 731.0 | 1.395349 | 0.544894 | 1.000000 | 1.000000 | 1.000000 | 2.000000 | 3.000000 | . temp 731.0 | 20.310776 | 7.505091 | 2.424346 | 13.820424 | 20.431653 | 26.872076 | 35.328347 | . atemp 731.0 | 23.717699 | 8.148059 | 3.953480 | 16.892125 | 24.336650 | 30.430100 | 42.044800 | . hum 731.0 | 62.789406 | 14.242910 | 0.000000 | 52.000000 | 62.666700 | 73.020850 | 97.250000 | . windspeed 731.0 | 19.048621 | 7.749787 | 2.239170 | 13.495000 | 18.097500 | 23.321450 | 50.746300 | . casual 731.0 | 848.176471 | 686.622488 | 2.000000 | 315.500000 | 713.000000 | 1096.000000 | 3410.000000 | . registered 731.0 | 3656.172367 | 1560.256377 | 20.000000 | 2497.000000 | 3662.000000 | 4776.500000 | 6946.000000 | . cnt 731.0 | 4504.348837 | 1937.211452 | 22.000000 | 3152.000000 | 4548.000000 | 5956.000000 | 8714.000000 | . Any missing data? . Good, there is no missing data. . df.isnull().values.any() . False . Visualization of time series . df[[&#39;temp&#39;, &#39;hum&#39;, &#39;windspeed&#39;]].plot(figsize=(10, 5)) df[[&#39;casual&#39;, &#39;registered&#39;]].plot(figsize=(10, 5)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb23c27e690&gt; . Cross Correlation Matrix . plt.subplots(figsize=(8, 6)) sns.heatmap(df.corr()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb23c42a090&gt; . I think the temp and atemp (feels like temperature) columns are too correlated, so perhaps we can discard atemp here. . Split the data . Let&#39;s make a simple manual split for now. . n = len(df) train_df = df[:int(0.8*n)].copy() test_df = df[int(0.8*n):].copy() . Normalize the data . We just need to normalize the non-categorical data. So, first we define them and then normalize those columns only. . non_cat = [&#39;temp&#39;, &#39;hum&#39;, &#39;windspeed&#39;, &#39;casual&#39;, &#39;registered&#39;, &#39;cnt&#39;] train_mean = train_df[non_cat].mean() train_std = train_df[non_cat].std() train_df.loc[:, non_cat] = (train_df.loc[:, non_cat] - train_mean) / train_std test_df.loc[:, non_cat] = (test_df.loc[:,non_cat] - train_mean) / train_std . Let&#39;s plot the probability density of the data using violin data to make sure our normalization looks good. . df_std = (df[non_cat] - train_mean) / train_std df_melt = df_std.melt(var_name=&#39;Column&#39;, value_name=&#39;Normalized&#39;) plt.figure(figsize=(12, 6)) ax = sns.violinplot(x=&#39;Column&#39;, y=&#39;Normalized&#39;, data=df_melt) _ = ax.set_xticklabels(df[non_cat].keys(), rotation=90) . Feature selection . There are two types of input data: . Seasonal data | Weather data | . and the output of the model is the bike user count which can be found in three columns of: . casual | registered | cnt (sum of the causual and registered) | . We can drop &#39;cnt&#39; as it&#39;s just the total count of users. I experimented with seasonal data only or weather data only as the input to the model. But, the performance was better when all input features are used. . . # separate input and output in each dataset input_columns = [&#39;season&#39;, &#39;yr&#39;, &#39;mnth&#39;, &#39;weekday&#39;, &#39;workingday&#39;,&#39;temp&#39;, &#39;hum&#39;, &#39;windspeed&#39;] output_columns = [&#39;casual&#39;, &#39;registered&#39;] train_in = train_df[input_columns].to_numpy() test_in = test_df[input_columns].to_numpy() train_out = train_df[output_columns].to_numpy() test_out = test_df[output_columns].to_numpy() print(f&#39;train_in shape: {train_in.shape}&#39;) print(f&#39;test_in shape: {test_in.shape}&#39;) print(f&#39;train_out shape: {train_out.shape}&#39;) print(f&#39;test_out shape: {test_out.shape}&#39;) . train_in shape: (584, 8) test_in shape: (147, 8) train_out shape: (584, 2) test_out shape: (147, 2) . Denormalizing function . Before evaluating the models, we invert the normalization and compare the predictions and true values in their actual scale. We use the following function to invert the normalization. . # mean and std to be used for inverting the normalizatioin _mean = train_mean[output_columns].values _std = train_std[output_columns].values # Denormalize the data def denorm(data, mean, std): data_denorm = data * std + mean return data_denorm . Models . Let&#39;s define a function to evaluate the models. . def evaluate_model(model, train_in, train_out, test_in, test_out): print(&#39; n***** Training performance: *****&#39;) train_pred = model.predict(train_in) train_pred_denorm = denorm(train_pred, _mean, _std) train_out_denorm = denorm(train_out, _mean, _std) train_mae = round(mean_absolute_error(train_pred_denorm, train_out_denorm), 2) print(&#39;MAE = &#39;, train_mae) print(&#39; n***** Testing performance: *****&#39;) test_pred = model.predict(test_in) test_pred_denorm = denorm(test_pred, _mean, _std) test_out_denorm = denorm(test_out, _mean, _std) test_mae = round(mean_absolute_error(test_pred_denorm, test_out_denorm), 2) print(&#39;MAE = &#39;, test_mae) for i, col in enumerate(output_columns): plt.figure(figsize=(10,5)) plt.plot(test_pred_denorm[:, i]) plt.plot(test_out_denorm[:, i]) plt.legend([&#39;predict&#39;, &#39;true&#39;]) plt.title(f&#39;Predict vs. ground truth for {col}&#39;) return train_mae, test_mae . Gradient Boosting . # Creating a MultiOutputRegressor object with GradientBoostingRegressor estimtor # This assumes that the outputs are independent of each other, which might not be a correct assumption. gbr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0)) print(&#39; n====== GradientBoostingRegressor =====&#39;) gbr.fit(train_in, train_out) gbr_train_mae, gbr_test_mae = evaluate_model(gbr, train_in, train_out, test_in, test_out) . ====== GradientBoostingRegressor ===== ***** Training performance: ***** MAE = 165.58 ***** Testing performance: ***** MAE = 515.7 . Random Forest . rfr = RandomForestRegressor(n_estimators=100, criterion=&#39;mae&#39;, random_state=0) print(&#39; n====== RandomForestRegressor =====&#39;) rfr.fit(train_in, train_out) rfr_train_mae, rfr_test_mae = evaluate_model(rfr, train_in, train_out, test_in, test_out) . ====== RandomForestRegressor ===== ***** Training performance: ***** MAE = 97.21 ***** Testing performance: ***** MAE = 548.42 . DNN . nn = tf.keras.Sequential([ tf.keras.layers.Dense(32, input_shape=(len(input_columns),)), tf.keras.layers.Dense(len(output_columns)) ]) print(nn.summary()) . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 32) 288 _________________________________________________________________ dense_1 (Dense) (None, 2) 66 ================================================================= Total params: 354 Trainable params: 354 Non-trainable params: 0 _________________________________________________________________ None . MAX_EPOCHS = 100 def compile_and_fit(model, train_in, train_out, test_in, test_out, patience=5): early_stopping = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_loss&#39;, patience=patience, mode=&#39;min&#39;) model.compile(loss=tf.losses.MeanSquaredError(), optimizer=tf.optimizers.Adam(), metrics=[tf.metrics.MeanAbsoluteError()]) history = model.fit(train_in, train_out, epochs=MAX_EPOCHS, validation_data=(test_in, test_out), callbacks=[early_stopping], batch_size = 32, verbose=0) return history . tf.keras.backend.clear_session() history = compile_and_fit(nn, train_in, train_out, test_in, test_out) plt.figure() plt.plot(history.history[&#39;loss&#39;]) plt.plot(history.history[&#39;val_loss&#39;]) plt.legend([&#39;loss&#39;, &#39;val_loss&#39;]) plt.figure() plt.plot(history.history[&#39;mean_absolute_error&#39;]) plt.plot(history.history[&#39;val_mean_absolute_error&#39;]) plt.legend([&#39;mean_absolute_error&#39;, &#39;val_mean_absolute_error&#39;]) . &lt;matplotlib.legend.Legend at 0x7fb23c76a590&gt; . nn_train_mae, nn_test_mae = evaluate_model(nn, train_in, train_out, test_in, test_out) . ***** Training performance: ***** MAE = 384.11 ***** Testing performance: ***** MAE = 559.46 . Comparing the models . print(&#39;Model: Train MAE - Test MAE&#39;) print(f&#39;Gradient Boosting: {gbr_train_mae} - {gbr_test_mae }&#39;) print(f&#39;Random Forest: {rfr_train_mae} - {rfr_test_mae }&#39;) print(f&#39;DNN: {nn_train_mae} - {nn_test_mae }&#39;) . Model: Train MAE - Test MAE Gradient Boosting: 165.58 - 515.7 Random Forest: 97.21 - 548.42 DNN: 384.11 - 559.46 . It looks like the best results based on test MAE is driven from Gradient Boosting method. Perhaps the other models specially DNN can be tuned to achieve at least the same level of performance. Please let me know if you have any suggestion to improve the models. Thanks for your feedback. .",
            "url": "https://sepidehdoost.com/prediction/2020/09/26/bike-sharing-count-prediction.html",
            "relUrl": "/prediction/2020/09/26/bike-sharing-count-prediction.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "How to Detect Fake and Real News with Natural Language Processing",
            "content": ". I have recently taken a great NLP course by deeplearning.ai (Andrew Ng) called &quot;Sequence Models&quot;. I always like to start a new project after learning a new skill. So, I found this interesting dataset in Kaggle Fake and real news dataset. The objective is to make an algorithm able to determine if an article is fake news or not. . I applied my learnings on this dataset and hope in this way I can share what I learned with you. . Solution Framework . There are two csv files in this Kaggle datatset each containing a list of articles considered as &quot;fake&quot; and &quot;real&quot; news. . The picture below shows the steps we will be taking in order. We&#39;ll explain the details of each step throughout the notebook. . . # Input data files are available in the &quot;../input/&quot; directory. # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # Any results you write to the current directory are saved as output. . /kaggle/input/fake-and-real-news-dataset/True.csv /kaggle/input/fake-and-real-news-dataset/Fake.csv /kaggle/input/glove42b300dtxt/glove.42B.300d.txt . Loading the required libraries . #for data analysis and modeling import tensorflow as tf from tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Dropout from tensorflow.keras.preprocessing import text, sequence from tensorflow.keras.models import Sequential from sklearn.model_selection import train_test_split import pandas as pd import numpy as np #for text cleaning import string import re import nltk from nltk.tokenize import word_tokenize from nltk.corpus import stopwords #for visualization import matplotlib.pyplot as plt . Loading data and visualizing . true = pd.read_csv(&#39;/kaggle/input/fake-and-real-news-dataset/True.csv&#39;) true.head() . title text subject date . 0 As U.S. budget fight looms, Republicans flip t... | WASHINGTON (Reuters) - The head of a conservat... | politicsNews | December 31, 2017 | . 1 U.S. military to accept transgender recruits o... | WASHINGTON (Reuters) - Transgender people will... | politicsNews | December 29, 2017 | . 2 Senior U.S. Republican senator: &#39;Let Mr. Muell... | WASHINGTON (Reuters) - The special counsel inv... | politicsNews | December 31, 2017 | . 3 FBI Russia probe helped by Australian diplomat... | WASHINGTON (Reuters) - Trump campaign adviser ... | politicsNews | December 30, 2017 | . 4 Trump wants Postal Service to charge &#39;much mor... | SEATTLE/WASHINGTON (Reuters) - President Donal... | politicsNews | December 29, 2017 | . fake = pd.read_csv(&#39;/kaggle/input/fake-and-real-news-dataset/Fake.csv&#39;) fake.head() . title text subject date . 0 Donald Trump Sends Out Embarrassing New Year’... | Donald Trump just couldn t wish all Americans ... | News | December 31, 2017 | . 1 Drunk Bragging Trump Staffer Started Russian ... | House Intelligence Committee Chairman Devin Nu... | News | December 31, 2017 | . 2 Sheriff David Clarke Becomes An Internet Joke... | On Friday, it was revealed that former Milwauk... | News | December 30, 2017 | . 3 Trump Is So Obsessed He Even Has Obama’s Name... | On Christmas day, Donald Trump announced that ... | News | December 29, 2017 | . 4 Pope Francis Just Called Out Donald Trump Dur... | Pope Francis used his annual Christmas Day mes... | News | December 25, 2017 | . Combine fake and true dataframes . Create a new column &#39;truth&#39; showing whethere the news is fake or real. Then, concatenate two datasets into one dataframe. We can choose to use either &#39;title&#39; or &#39;text&#39; column or concatenated &#39;title+text&#39; for training. But, for the sake of processing time, we&#39;ll only use &#39;title&#39;. . true[&#39;truth&#39;] = 1 fake[&#39;truth&#39;] = 0 df = pd.concat([true, fake], axis=0, ignore_index=True) df.shape . (44898, 5) . Text Cleaning . We need to clean the text first. If you start searching on the text cleaning domain, you realize there are many different techniques. But you may need just a few methods for the purpose of your NLP task. There are many good resources for text clearning. But, I mainly looked up the following resources for my text cleaning in this notebook: https://machinelearningmastery.com/clean-text-machine-learning-python/ https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/ Here are 5 steps that give decent text cleaning result for this task: . Replace contractions In English, a contraction is a word or phrase that has been shortened by dropping one or more letters, such as “I’m” instead of “I am”. We can either split the contractions (“I’m” to “ I ”+” ’m ”) or convert them to their full format (“I’m ” to “I am”). In my experience the latter works better as it&#39;s harder to find a word embedding for sub-words like “ ‘m “. | Removing punctuation We want the sentences without punctuations like commas, brackets, etc. Python has a constant called string.punctuation that provides a list of punctuation characters. We’ll use this list to clean our text from punctuations. | Splitting into words In order to remove stopwords, we first need to split the text into words. We do this with word_tokenize function by NLTK. This function splits the text based on white space and punctuation. | Removing stopwords Stopwords are common words like “the”, “and”, … which don’t add much value to the meaning of the text. NLTK has a list of these words that can be imported and used to remove them from the text. | Removing leftover punctuations I noticed after all this cleaning, there were still some words like “…but” with dots in them. I added this last step to clean them up. Normalizing by case is also common practice. But, since we are using keras tokenizer later, we can skip this step as tokenizer does this step by default. There are other preprocessing techniques of text like Stemming, and Lemmatization. However, in the realm of deep learning NLP they are not necessary anymore. | %%time def clean_text(txt): &quot;&quot;&quot;&quot;&quot; cleans the input text in the following steps 1- replace contractions 2- removing punctuation 3- spliting into words 4- removing stopwords 5- removing leftover punctuations &quot;&quot;&quot;&quot;&quot; contraction_dict = {&quot;ain&#39;t&quot;: &quot;is not&quot;, &quot;aren&#39;t&quot;: &quot;are not&quot;,&quot;can&#39;t&quot;: &quot;cannot&quot;, &quot;&#39;cause&quot;: &quot;because&quot;, &quot;could&#39;ve&quot;: &quot;could have&quot;, &quot;couldn&#39;t&quot;: &quot;could not&quot;, &quot;didn&#39;t&quot;: &quot;did not&quot;, &quot;doesn&#39;t&quot;: &quot;does not&quot;, &quot;don&#39;t&quot;: &quot;do not&quot;, &quot;hadn&#39;t&quot;: &quot;had not&quot;, &quot;hasn&#39;t&quot;: &quot;has not&quot;, &quot;haven&#39;t&quot;: &quot;have not&quot;, &quot;he&#39;d&quot;: &quot;he would&quot;,&quot;he&#39;ll&quot;: &quot;he will&quot;, &quot;he&#39;s&quot;: &quot;he is&quot;, &quot;how&#39;d&quot;: &quot;how did&quot;, &quot;how&#39;d&#39;y&quot;: &quot;how do you&quot;, &quot;how&#39;ll&quot;: &quot;how will&quot;, &quot;how&#39;s&quot;: &quot;how is&quot;, &quot;I&#39;d&quot;: &quot;I would&quot;, &quot;I&#39;d&#39;ve&quot;: &quot;I would have&quot;, &quot;I&#39;ll&quot;: &quot;I will&quot;, &quot;I&#39;ll&#39;ve&quot;: &quot;I will have&quot;,&quot;I&#39;m&quot;: &quot;I am&quot;, &quot;I&#39;ve&quot;: &quot;I have&quot;, &quot;i&#39;d&quot;: &quot;i would&quot;, &quot;i&#39;d&#39;ve&quot;: &quot;i would have&quot;, &quot;i&#39;ll&quot;: &quot;i will&quot;, &quot;i&#39;ll&#39;ve&quot;: &quot;i will have&quot;,&quot;i&#39;m&quot;: &quot;i am&quot;, &quot;i&#39;ve&quot;: &quot;i have&quot;, &quot;isn&#39;t&quot;: &quot;is not&quot;, &quot;it&#39;d&quot;: &quot;it would&quot;, &quot;it&#39;d&#39;ve&quot;: &quot;it would have&quot;, &quot;it&#39;ll&quot;: &quot;it will&quot;, &quot;it&#39;ll&#39;ve&quot;: &quot;it will have&quot;,&quot;it&#39;s&quot;: &quot;it is&quot;, &quot;let&#39;s&quot;: &quot;let us&quot;, &quot;ma&#39;am&quot;: &quot;madam&quot;, &quot;mayn&#39;t&quot;: &quot;may not&quot;, &quot;might&#39;ve&quot;: &quot;might have&quot;,&quot;mightn&#39;t&quot;: &quot;might not&quot;,&quot;mightn&#39;t&#39;ve&quot;: &quot;might not have&quot;, &quot;must&#39;ve&quot;: &quot;must have&quot;, &quot;mustn&#39;t&quot;: &quot;must not&quot;, &quot;mustn&#39;t&#39;ve&quot;: &quot;must not have&quot;, &quot;needn&#39;t&quot;: &quot;need not&quot;, &quot;needn&#39;t&#39;ve&quot;: &quot;need not have&quot;,&quot;o&#39;clock&quot;: &quot;of the clock&quot;, &quot;oughtn&#39;t&quot;: &quot;ought not&quot;, &quot;oughtn&#39;t&#39;ve&quot;: &quot;ought not have&quot;, &quot;shan&#39;t&quot;: &quot;shall not&quot;, &quot;sha&#39;n&#39;t&quot;: &quot;shall not&quot;, &quot;shan&#39;t&#39;ve&quot;: &quot;shall not have&quot;, &quot;she&#39;d&quot;: &quot;she would&quot;, &quot;she&#39;d&#39;ve&quot;: &quot;she would have&quot;, &quot;she&#39;ll&quot;: &quot;she will&quot;, &quot;she&#39;ll&#39;ve&quot;: &quot;she will have&quot;, &quot;she&#39;s&quot;: &quot;she is&quot;, &quot;should&#39;ve&quot;: &quot;should have&quot;, &quot;shouldn&#39;t&quot;: &quot;should not&quot;, &quot;shouldn&#39;t&#39;ve&quot;: &quot;should not have&quot;, &quot;so&#39;ve&quot;: &quot;so have&quot;,&quot;so&#39;s&quot;: &quot;so as&quot;, &quot;this&#39;s&quot;: &quot;this is&quot;,&quot;that&#39;d&quot;: &quot;that would&quot;, &quot;that&#39;d&#39;ve&quot;: &quot;that would have&quot;, &quot;that&#39;s&quot;: &quot;that is&quot;, &quot;there&#39;d&quot;: &quot;there would&quot;, &quot;there&#39;d&#39;ve&quot;: &quot;there would have&quot;, &quot;there&#39;s&quot;: &quot;there is&quot;, &quot;here&#39;s&quot;: &quot;here is&quot;,&quot;they&#39;d&quot;: &quot;they would&quot;, &quot;they&#39;d&#39;ve&quot;: &quot;they would have&quot;, &quot;they&#39;ll&quot;: &quot;they will&quot;, &quot;they&#39;ll&#39;ve&quot;: &quot;they will have&quot;, &quot;they&#39;re&quot;: &quot;they are&quot;, &quot;they&#39;ve&quot;: &quot;they have&quot;, &quot;to&#39;ve&quot;: &quot;to have&quot;, &quot;wasn&#39;t&quot;: &quot;was not&quot;, &quot;we&#39;d&quot;: &quot;we would&quot;, &quot;we&#39;d&#39;ve&quot;: &quot;we would have&quot;, &quot;we&#39;ll&quot;: &quot;we will&quot;, &quot;we&#39;ll&#39;ve&quot;: &quot;we will have&quot;, &quot;we&#39;re&quot;: &quot;we are&quot;, &quot;we&#39;ve&quot;: &quot;we have&quot;, &quot;weren&#39;t&quot;: &quot;were not&quot;, &quot;what&#39;ll&quot;: &quot;what will&quot;, &quot;what&#39;ll&#39;ve&quot;: &quot;what will have&quot;, &quot;what&#39;re&quot;: &quot;what are&quot;, &quot;what&#39;s&quot;: &quot;what is&quot;, &quot;what&#39;ve&quot;: &quot;what have&quot;, &quot;when&#39;s&quot;: &quot;when is&quot;, &quot;when&#39;ve&quot;: &quot;when have&quot;, &quot;where&#39;d&quot;: &quot;where did&quot;, &quot;where&#39;s&quot;: &quot;where is&quot;, &quot;where&#39;ve&quot;: &quot;where have&quot;, &quot;who&#39;ll&quot;: &quot;who will&quot;, &quot;who&#39;ll&#39;ve&quot;: &quot;who will have&quot;, &quot;who&#39;s&quot;: &quot;who is&quot;, &quot;who&#39;ve&quot;: &quot;who have&quot;, &quot;why&#39;s&quot;: &quot;why is&quot;, &quot;why&#39;ve&quot;: &quot;why have&quot;, &quot;will&#39;ve&quot;: &quot;will have&quot;, &quot;won&#39;t&quot;: &quot;will not&quot;, &quot;won&#39;t&#39;ve&quot;: &quot;will not have&quot;, &quot;would&#39;ve&quot;: &quot;would have&quot;, &quot;wouldn&#39;t&quot;: &quot;would not&quot;, &quot;wouldn&#39;t&#39;ve&quot;: &quot;would not have&quot;, &quot;y&#39;all&quot;: &quot;you all&quot;, &quot;y&#39;all&#39;d&quot;: &quot;you all would&quot;,&quot;y&#39;all&#39;d&#39;ve&quot;: &quot;you all would have&quot;,&quot;y&#39;all&#39;re&quot;: &quot;you all are&quot;,&quot;y&#39;all&#39;ve&quot;: &quot;you all have&quot;,&quot;you&#39;d&quot;: &quot;you would&quot;, &quot;you&#39;d&#39;ve&quot;: &quot;you would have&quot;, &quot;you&#39;ll&quot;: &quot;you will&quot;, &quot;you&#39;ll&#39;ve&quot;: &quot;you will have&quot;, &quot;you&#39;re&quot;: &quot;you are&quot;, &quot;you&#39;ve&quot;: &quot;you have&quot;} def _get_contractions(contraction_dict): contraction_re = re.compile(&#39;(%s)&#39; % &#39;|&#39;.join(contraction_dict.keys())) return contraction_dict, contraction_re def replace_contractions(text): contractions, contractions_re = _get_contractions(contraction_dict) def replace(match): return contractions[match.group(0)] return contractions_re.sub(replace, text) # replace contractions txt = replace_contractions(txt) #remove punctuations txt = &quot;&quot;.join([char for char in txt if char not in string.punctuation]) txt = re.sub(&#39;[0-9]+&#39;, &#39;&#39;, txt) # split into words words = word_tokenize(txt) # remove stopwords stop_words = set(stopwords.words(&#39;english&#39;)) words = [w for w in words if not w in stop_words] # removing leftover punctuations words = [word for word in words if word.isalpha()] cleaned_text = &#39; &#39;.join(words) return cleaned_text df[&#39;data_cleaned&#39;] = df[&#39;title&#39;].apply(lambda txt: clean_text(txt)) . CPU times: user 20.4 s, sys: 751 ms, total: 21.1 s Wall time: 21.2 s . df[&#39;data_cleaned&#39;] . 0 As US budget fight looms Republicans flip fisc... 1 US military accept transgender recruits Monday... 2 Senior US Republican senator Let Mr Mueller job 3 FBI Russia probe helped Australian diplomat ti... 4 Trump wants Postal Service charge much Amazon ... ... 44893 McPain John McCain Furious That Iran Treated U... 44894 JUSTICE Yahoo Settles Email Privacy Classactio... 44895 Sunnistan US Allied Safe Zone Plan Take Territ... 44896 How Blow Million Al Jazeera America Finally Ca... 44897 US Navy Sailors Held Iranian Military Signs Ne... Name: data_cleaned, Length: 44898, dtype: object . Prepare train and test datasets . Use the usual train_test_split by sklearn to split the data. . xtrain, xtest, ytrain, ytest = train_test_split(df[&#39;data_cleaned&#39;], df[&#39;truth&#39;], shuffle=True, test_size=0.2) # find the length of the largest sentence in training data max_len = xtrain.apply(lambda x: len(x)).max() print(f&#39;Max number of words in a text in training data: {max_len}&#39;) . Max number of words in a text in training data: 227 . Tokenize the input training sentences . In most of the NLP tasks, we need to represent each word in the text with an integer value (index) before feeding it to any model. In this way, the text will be converted to a sequence of integer values. One of the ways of doing that is with Keras. Keras provides an API for tokenizing the text. Tokenizer in Keras finds the frequency of each unique word and sort them based on their frequency. It then assigns an integer value starting from 1 to each word from the top. You can see the index mapping dictionary by reading tokenizer.word_index. . There are two distinct steps in tokenizing in this way: . fit_on_texts: We&#39;ll fit the tokenizer on our training data to create the word indices | texts_to_sequences: using the word index dictionary from step above, we take this step to transform both train and test data. | Here, we set num_words to a limit such as 10000 words. num_words is a parameter that defines the maximum number of words to keep, based on the word frequency. Keras actually keeps (num_words-1) words. We can leave the num_words to &#39;None&#39; and tokenizer will pick all the words in the vacabulary. . Padding and truncating the input training sequences . All your input sequences to the model need to have the same length. In order to achieve that, we can use a function that pads the short sequences with zeros (options are &#39;pre&#39; or &#39;post&#39; which pads either before or after each sequence). It also truncates any sequence that is longer than a predefined parameter &quot;maxlen&quot;. Truncating also has the option of &#39;pre&#39; or &#39;post&#39; which either truncates at the beginning or at the end of the sequences. . max_words = 10000 tokenizer = text.Tokenizer(num_words = max_words) # create the vocabulary by fitting on x_train text tokenizer.fit_on_texts(xtrain) # generate the sequence of tokens xtrain_seq = tokenizer.texts_to_sequences(xtrain) xtest_seq = tokenizer.texts_to_sequences(xtest) # pad the sequences xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len) xtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len) word_index = tokenizer.word_index print(&#39;text example:&#39;, xtrain[0]) print(&#39;sequence of indices(before padding):&#39;, xtrain_seq[0]) print(&#39;sequence of indices(after padding):&#39;, xtrain_pad[0]) . text example: As US budget fight looms Republicans flip fiscal script sequence of indices(before padding): [47, 2455, 615, 996, 3704, 118, 2692, 3534] sequence of indices(after padding): [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 47 2455 615 996 3704 118 2692 3534] . Word embedding using pre-trained GloVe vectors . Now that we have tokenized the text, we use GloVe pretrained vectors. Word embeddings is a way to represent words with similar meaning to have a similar representation.  Using word embedding through GloVe, we can have a decent performance with models with even relatively small label training sets. . You can download GloVe pre-trained word vectors from the link below. There are different sizes of vocabulary and embedding dimension available. . https://nlp.stanford.edu/projects/glove/ . Load the GloVe vectors . %%time embedding_vectors = {} # with open(&#39;/kaggle/input/glove6b100d/glove.6B.100d.txt&#39;,&#39;r&#39;,encoding=&#39;utf-8&#39;) as file: with open(&#39;/kaggle/input/glove42b300dtxt/glove.42B.300d.txt&#39;,&#39;r&#39;,encoding=&#39;utf-8&#39;) as file: for row in file: values = row.split(&#39; &#39;) word = values[0] weights = np.asarray([float(val) for val in values[1:]]) embedding_vectors[word] = weights print(f&quot;Size of vocabulary in GloVe: {len(embedding_vectors)}&quot;) . Size of vocabulary in GloVe: 1917494 CPU times: user 4min 21s, sys: 6.44 s, total: 4min 27s Wall time: 4min 27s . Create an embedding matrix with the GloVe vectors . The embedding matrix has a shape of (vocabulary length, embedding dimension). . Note tht vacab_len is equal to max_words if we set that limit when tokenizing. Otherwise, vocab_len is equal to lenght of all words in word_index+1. . Each row of the embedding matrix belongs to one word in the vocabulary (derived from xtrain) and it contains the weights of embedding vector of that word. . In the code below, we initialze the embedding matrix with zeros. If a word in our word_index is not found in the embedding vectors from GloVe. . The weight of that word remains as zero. Below, you can see print out of the some example words that are out of vacabulary (OOV). . #initialize the embedding_matrix with zeros emb_dim = 300 if max_words is not None: vocab_len = max_words else: vocab_len = len(word_index)+1 embedding_matrix = np.zeros((vocab_len, emb_dim)) oov_count = 0 oov_words = [] for word, idx in word_index.items(): if idx &lt; vocab_len: embedding_vector = embedding_vectors.get(word) if embedding_vector is not None: embedding_matrix[idx] = embedding_vector else: oov_count += 1 oov_words.append(word) #print some of the out of vocabulary words print(f&#39;Some out of valubulary words: {oov_words[0:5]}&#39;) . Some out of valubulary words: [&#39;brexit&#39;, &#39;antitrump&#39;, &#39;reutersipsos&#39;, &#39;protrump&#39;, &#39;blacklivesmatter&#39;] . print(f&#39;{oov_count} out of {vocab_len} words were OOV.&#39;) . 210 out of 10000 words were OOV. . Modeling . Now, we can create a model and pre-train the embedding layer with the embedding matrix we just created based on GloVe vectors. You can see the model overview below. . The model architecture looks like the picture below. Although tokenizing is not part of the modeling, it&#39;s nice to show that this step is taken before feeding the text data to the LSTM models. This model is similar to one of the programming assignments in the &quot;Sequence model&quot; course. But, one of the LSTM layers are removed as it turned out that adding that extra layer doesn&#39;t help and even cause some overfitting. . The first layer of the model is Embedding. Embedding layer by Keras is a flexible layer that can be used also without any pre-trained weights. In that case, the Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset. In this exercise, we will set the weights of the Embedding layer to the embedding matrix from GloVe pre-trained vectors.This is a tranfer learning. . Another parameter in Embedding layer is &quot;trainable&quot; which can be set to True in case you want to fine-tune the word embedding or if you don&#39;t want the embedding weights to be updated you can set it to False. Here, we set it to False. . After the Embedding layer, we have a layer of LSTM or GRU and then a Dropout layer for regularization. . Then we have a Dense layer with Sigmoid activation which transforms the output of previous layers to 0 or 1 (real or fake). . . Model Overview . LSTM . Let&#39;s start with LSTM models. . lstm_model = Sequential() lstm_model.add(Embedding(vocab_len, emb_dim, trainable = False, weights=[embedding_matrix])) lstm_model.add(LSTM(128, return_sequences=False)) lstm_model.add(Dropout(0.5)) lstm_model.add(Dense(1, activation = &#39;sigmoid&#39;)) lstm_model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) print(lstm_model.summary()) . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, None, 300) 3000000 _________________________________________________________________ lstm (LSTM) (None, 128) 219648 _________________________________________________________________ dropout (Dropout) (None, 128) 0 _________________________________________________________________ dense (Dense) (None, 1) 129 ================================================================= Total params: 3,219,777 Trainable params: 219,777 Non-trainable params: 3,000,000 _________________________________________________________________ None . %%time batch_size = 256 epochs = 10 history = lstm_model.fit(xtrain_pad, np.asarray(ytrain), validation_data=(xtest_pad, np.asarray(ytest)), batch_size = batch_size, epochs = epochs) . Train on 35918 samples, validate on 8980 samples Epoch 1/10 35918/35918 [==============================] - 12s 330us/sample - loss: 0.1254 - accuracy: 0.9511 - val_loss: 0.0745 - val_accuracy: 0.9731 Epoch 2/10 35918/35918 [==============================] - 6s 179us/sample - loss: 0.0572 - accuracy: 0.9798 - val_loss: 0.0554 - val_accuracy: 0.9812 Epoch 3/10 35918/35918 [==============================] - 6s 179us/sample - loss: 0.0431 - accuracy: 0.9850 - val_loss: 0.0575 - val_accuracy: 0.9791 Epoch 4/10 35918/35918 [==============================] - 6s 180us/sample - loss: 0.0324 - accuracy: 0.9894 - val_loss: 0.0479 - val_accuracy: 0.9827 Epoch 5/10 35918/35918 [==============================] - 7s 185us/sample - loss: 0.0246 - accuracy: 0.9921 - val_loss: 0.0436 - val_accuracy: 0.9852 Epoch 6/10 35918/35918 [==============================] - 7s 188us/sample - loss: 0.0184 - accuracy: 0.9940 - val_loss: 0.0432 - val_accuracy: 0.9853 Epoch 7/10 35918/35918 [==============================] - 6s 180us/sample - loss: 0.0116 - accuracy: 0.9965 - val_loss: 0.0443 - val_accuracy: 0.9875 Epoch 8/10 35918/35918 [==============================] - 6s 179us/sample - loss: 0.0120 - accuracy: 0.9962 - val_loss: 0.0498 - val_accuracy: 0.9862 Epoch 9/10 35918/35918 [==============================] - 6s 180us/sample - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.0519 - val_accuracy: 0.9875 Epoch 10/10 35918/35918 [==============================] - 6s 180us/sample - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.0530 - val_accuracy: 0.9878 CPU times: user 56.6 s, sys: 8.17 s, total: 1min 4s Wall time: 1min 10s . LSTM - Evaluation . Let&#39;s find the accuracy of training and testing dataset below: . #plot accuracy plt.figure(figsize=(15, 7)) plt.plot(range(epochs), history.history[&#39;accuracy&#39;]) plt.plot(range(epochs), history.history[&#39;val_accuracy&#39;]) plt.legend([&#39;training_acc&#39;, &#39;validation_acc&#39;]) plt.title(&#39;Accuracy&#39;) . Text(0.5, 1.0, &#39;Accuracy&#39;) . train_lstm_results = lstm_model.evaluate(xtrain_pad, np.asarray(ytrain), verbose=0, batch_size=256) test_lstm_results = lstm_model.evaluate(xtest_pad, np.asarray(ytest), verbose=0, batch_size=256) print(f&#39;Train accuracy: {train_lstm_results[1]*100:0.2f}&#39;) print(f&#39;Test accuracy: {test_lstm_results[1]*100:0.2f}&#39;) . Train accuracy: 99.92 Test accuracy: 98.78 . GRU . emb_dim = embedding_matrix.shape[1] gru_model = Sequential() gru_model.add(Embedding(vocab_len, emb_dim, trainable = False, weights=[embedding_matrix])) gru_model.add(GRU(128, return_sequences=False)) gru_model.add(Dropout(0.5)) gru_model.add(Dense(1, activation = &#39;sigmoid&#39;)) gru_model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) print(gru_model.summary()) . Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_1 (Embedding) (None, None, 300) 3000000 _________________________________________________________________ gru (GRU) (None, 128) 165120 _________________________________________________________________ dropout_1 (Dropout) (None, 128) 0 _________________________________________________________________ dense_1 (Dense) (None, 1) 129 ================================================================= Total params: 3,165,249 Trainable params: 165,249 Non-trainable params: 3,000,000 _________________________________________________________________ None . batch_size = 256 epochs = 10 history = gru_model.fit(xtrain_pad, np.asarray(ytrain), validation_data=(xtest_pad, np.asarray(ytest)), batch_size = batch_size, epochs = epochs) . Train on 35918 samples, validate on 8980 samples Epoch 1/10 35918/35918 [==============================] - 8s 226us/sample - loss: 0.1344 - accuracy: 0.9450 - val_loss: 0.0675 - val_accuracy: 0.9762 Epoch 2/10 35918/35918 [==============================] - 6s 179us/sample - loss: 0.0587 - accuracy: 0.9803 - val_loss: 0.0536 - val_accuracy: 0.9820 Epoch 3/10 35918/35918 [==============================] - 7s 184us/sample - loss: 0.0445 - accuracy: 0.9855 - val_loss: 0.0480 - val_accuracy: 0.9836 Epoch 4/10 35918/35918 [==============================] - 7s 183us/sample - loss: 0.0323 - accuracy: 0.9893 - val_loss: 0.0411 - val_accuracy: 0.9856 Epoch 5/10 35918/35918 [==============================] - 6s 179us/sample - loss: 0.0227 - accuracy: 0.9922 - val_loss: 0.0392 - val_accuracy: 0.9880 Epoch 6/10 35918/35918 [==============================] - 6s 180us/sample - loss: 0.0163 - accuracy: 0.9951 - val_loss: 0.0390 - val_accuracy: 0.9875 Epoch 7/10 35918/35918 [==============================] - 6s 178us/sample - loss: 0.0125 - accuracy: 0.9959 - val_loss: 0.0406 - val_accuracy: 0.9882 Epoch 8/10 35918/35918 [==============================] - 6s 177us/sample - loss: 0.0111 - accuracy: 0.9963 - val_loss: 0.0446 - val_accuracy: 0.9871 Epoch 9/10 35918/35918 [==============================] - 7s 182us/sample - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.0544 - val_accuracy: 0.9863 Epoch 10/10 35918/35918 [==============================] - 6s 177us/sample - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0479 - val_accuracy: 0.9888 . GRU Evaluation . Let&#39;s find the accuracy of training and testing dataset with GRU model below: . #plot accuracy plt.figure(figsize=(15, 7)) plt.plot(range(epochs), history.history[&#39;accuracy&#39;]) plt.plot(range(epochs), history.history[&#39;val_accuracy&#39;]) plt.legend([&#39;training_acc&#39;, &#39;validation_acc&#39;]) plt.title(&#39;Accuracy&#39;) . Text(0.5, 1.0, &#39;Accuracy&#39;) . train_gru_results = gru_model.evaluate(xtrain_pad, np.asarray(ytrain), verbose=0, batch_size=256) test_gru_results = gru_model.evaluate(xtest_pad, np.asarray(ytest), verbose=0, batch_size=256) print(f&#39;Train accuracy: {train_gru_results[1]*100:0.2f}&#39;) print(f&#39;Test accuracy: {test_gru_results[1]*100:0.2f}&#39;) . Train accuracy: 99.96 Test accuracy: 98.88 . As you can see in this example both GRU and LSTM perform similarly. Given the lower complexity of GRU, perhaps GRU is a better choice here. . There you have it. You learned all steps required to write a text classifier using pre-trained word embeddings such as GloVe and an LSTM or GRU model. . Let me know if you have any suggestions to improve this notebook. Thank you. .",
            "url": "https://sepidehdoost.com/nlp/2020/06/06/text-classification-with-glove-lstm-gru-99-acc.html",
            "relUrl": "/nlp/2020/06/06/text-classification-with-glove-lstm-gru-99-acc.html",
            "date": " • Jun 6, 2020"
        }
        
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi and welcome! I’m Sepideh, and I’m currently an AI and DSP Algorithm Designer in Ericsson Canada. I’m a MSc graduate from Chalmers University of Technology (located in beautiful Göteborg, Sweden), where I majored in Digital Signal Processing and Wireless Communications. . This blog is mainly a place to record and detail my learnings and I’m hoping that would help someone out there. That being said, I’m still a learner and if you have anything to point out, please leave them in the comments! I would love to hear your feedback and ideas. .",
          "url": "https://sepidehdoost.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sepidehdoost.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}